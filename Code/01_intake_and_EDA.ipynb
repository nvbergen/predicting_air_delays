{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "766eb4fc-473b-4a0e-ad7e-3d1d0f6e9282",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Predicting Air Delays \n",
    "----\n",
    "\n",
    "Thank you for your review of my code notebook.\n",
    "This notebook's goal is to obtain, modify, clean, and prepare the dataset for exposure on a Machine Learning algorithm.\n",
    "\n",
    "---\n",
    "#### Problem Statement: \n",
    "Both travelers and airlines find delays frustrating and costly. This project attempts to be able to predict the probability of a commercial flight delay for any flight in the United States. \n",
    "\n",
    "---\n",
    "\n",
    "#### MVP:\n",
    "My product will be a small lightweight application run on `streamlit` platform for proof-of-concept where a user can find the probability of their desired flight having a delay, how long the delay may be, and how much will the delay cost the user in _lost time_ at the destination \n",
    "\n",
    "---\n",
    "# Intake, Cleaning, and EDA. \n",
    "\n",
    "The primary challenge in this notebook is managing a large dataset. \n",
    "The next challenge will be to conduct meaningful EDA across the whole dataset. \n",
    "The notebook is structured as follows. \n",
    "1. Imports and set up\n",
    "2. The size and complexity issue. \n",
    "3. Cleaning steps. \n",
    "4. Feature engineering and selection. \n",
    "5. Save the final CSV and discuss next steps. \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a7a039-e68d-42d2-ae8e-aac54bc196a6",
   "metadata": {},
   "source": [
    "## 1. Set-up\n",
    "----\n",
    "I will be making use of `os`, `glob`, and `Amadeus API` libraries for python. \n",
    "\n",
    "`os` and `glob` will be used in conjunction with command line commands from the notebook to join the large CSV tables together.\n",
    "\n",
    "`amadeus` is used as a way to utilize the service's self-service APIs. The API requires a token /key to use. \n",
    "[**sign up here**](https://developers.amadeus.com)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156c8c23-771c-4565-aae9-217d6ecf6870",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install amadeus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1cf4036e-d2dc-4772-ab16-d833e8e69624",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from amadeus import Client, ResponseError\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a641813-7982-42c1-9f4f-cc8d94fbbd51",
   "metadata": {},
   "source": [
    " I have 68 CSV files. Each file represents 1 month of flight history from all U.S. Airports. As a result each CSV is approximatley 150mb in size. Loading a few of them into the workspace of the notebook will result in a loss of data due to data exceeding memory capacity on the local machine. \n",
    " <br>\n",
    " <br>\n",
    "The approach will be to manipulate each of the CSV's and join them directly in the command line. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e87f7c0f-bf35-48ac-a29b-bdc9a503cfe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#change the directory from root to where all the files I will join will be\n",
    "os.chdir('/Volumes/lacie/data_ingestion/capstone_hopper')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44cc0827-9b7e-41f8-add9-ff7887813038",
   "metadata": {},
   "source": [
    "### Data Sources\n",
    "---\n",
    "This project attempts gathered delay data from the **Department of Transportation (DOT) Flight Delay reporting Database**. Sadly, there was no public API available to access this data from DOT or from Federal Aviation Administration.\n",
    "\n",
    "Given there was no way to programatically acquire the desired amount of data, I proceeded to utilize the basic public data library tool and download a CSV for one monthly period at a time.  \n",
    "\n",
    "This created a lot of _just **too big** files_ and hence our first unanticipated technical challenge with this project; what do I do? \n",
    "\n",
    "The plan: use the command line to join all the tables. \n",
    "After cleaning see how large the file is. \n",
    "\n",
    "To implement this plan, using `glob` methods and direct command line. \n",
    "\n",
    "\n",
    "---\n",
    "References<br>\n",
    "[Bureau of Transportation Statistics](https://www.transtats.bts.gov/DL_SelectFields.asp?gnoyr_VQ=FGJ)\n",
    "<br>\n",
    "[GLOB tutorial](https://www.freecodecamp.org/news/how-to-combine-multiple-csv-files-with-8-lines-of-code-265183e0854/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e7b3315-9cc1-4042-80c9-1c1f9ce8ec6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m10_nov_20.csv\u001b[m\u001b[m \u001b[31m23_oct_19.csv\u001b[m\u001b[m \u001b[31m36_sep_18.csv\u001b[m\u001b[m \u001b[31m49_aug_17.csv\u001b[m\u001b[m \u001b[31m61_aug_16.csv\u001b[m\u001b[m\n",
      "\u001b[31m11_oct_20.csv\u001b[m\u001b[m \u001b[31m24_sep_19.csv\u001b[m\u001b[m \u001b[31m37_aug_18.csv\u001b[m\u001b[m \u001b[31m4_may_21.csv\u001b[m\u001b[m  \u001b[31m62_jul_16.csv\u001b[m\u001b[m\n",
      "\u001b[31m12_sep_20.csv\u001b[m\u001b[m \u001b[31m25_aug_19.csv\u001b[m\u001b[m \u001b[31m38_jul_18.csv\u001b[m\u001b[m \u001b[31m50_jul_17.csv\u001b[m\u001b[m \u001b[31m63_jun_16.csv\u001b[m\u001b[m\n",
      "\u001b[31m13_aug_20.csv\u001b[m\u001b[m \u001b[31m26_jul_19.csv\u001b[m\u001b[m \u001b[31m39_jun_18.csv\u001b[m\u001b[m \u001b[31m51_jun_17.csv\u001b[m\u001b[m \u001b[31m64_may_16.csv\u001b[m\u001b[m\n",
      "\u001b[31m14_jul_20.csv\u001b[m\u001b[m \u001b[31m27_jun_19.csv\u001b[m\u001b[m \u001b[31m3_jun_21.csv\u001b[m\u001b[m  \u001b[31m52_may_17.csv\u001b[m\u001b[m \u001b[31m65_apr_16.csv\u001b[m\u001b[m\n",
      "\u001b[31m15_jun_20.csv\u001b[m\u001b[m \u001b[31m28_may_19.csv\u001b[m\u001b[m \u001b[31m40_may_18.csv\u001b[m\u001b[m \u001b[31m53_apr_17.csv\u001b[m\u001b[m \u001b[31m66_mar_16.csv\u001b[m\u001b[m\n",
      "\u001b[31m16_may_20.csv\u001b[m\u001b[m \u001b[31m29_apr_19.csv\u001b[m\u001b[m \u001b[31m41_apr_18.csv\u001b[m\u001b[m \u001b[31m54_mar_17.csv\u001b[m\u001b[m \u001b[31m67_feb_16.csv\u001b[m\u001b[m\n",
      "\u001b[31m17_apr_20.csv\u001b[m\u001b[m \u001b[31m2_jul_21.csv\u001b[m\u001b[m  \u001b[31m42_mar_18.csv\u001b[m\u001b[m \u001b[31m55_feb_17.csv\u001b[m\u001b[m \u001b[31m68_jan_16.csv\u001b[m\u001b[m\n",
      "\u001b[31m18_mar_20.csv\u001b[m\u001b[m \u001b[31m30_mar_19.csv\u001b[m\u001b[m \u001b[31m43_feb_18.csv\u001b[m\u001b[m \u001b[31m56_jan_17.csv\u001b[m\u001b[m \u001b[31m6_mar_21.csv\u001b[m\u001b[m\n",
      "\u001b[31m19_feb_20.csv\u001b[m\u001b[m \u001b[31m31_feb_19.csv\u001b[m\u001b[m \u001b[31m44_jan_18.csv\u001b[m\u001b[m \u001b[31m57_dec_16.csv\u001b[m\u001b[m \u001b[31m7_feb_21.csv\u001b[m\u001b[m\n",
      "\u001b[31m1_aug_21.csv\u001b[m\u001b[m  \u001b[31m32_jan_19.csv\u001b[m\u001b[m \u001b[31m45_dec_17.csv\u001b[m\u001b[m \u001b[31m58_nov_16.csv\u001b[m\u001b[m \u001b[31m8_jan_21.csv\u001b[m\u001b[m\n",
      "\u001b[31m20_jan_20.csv\u001b[m\u001b[m \u001b[31m33_dec_18.csv\u001b[m\u001b[m \u001b[31m46_nov_17.csv\u001b[m\u001b[m \u001b[31m59_oct_16.csv\u001b[m\u001b[m \u001b[31m9_dec_20.csv\u001b[m\u001b[m\n",
      "\u001b[31m21_dec_19.csv\u001b[m\u001b[m \u001b[31m34_nov_18.csv\u001b[m\u001b[m \u001b[31m47_oct_17.csv\u001b[m\u001b[m \u001b[31m5_apr_21.csv\u001b[m\u001b[m\n",
      "\u001b[31m22_nov_19.csv\u001b[m\u001b[m \u001b[31m35_oct_18.csv\u001b[m\u001b[m \u001b[31m48_sep_17.csv\u001b[m\u001b[m \u001b[31m60_sep_16.csv\u001b[m\u001b[m\n"
     ]
    }
   ],
   "source": [
    "#a list of all the files. a total of 6.958GB of unfiltered raw data. \n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0559f5e4-8774-41f1-947a-b9b2c94f70cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#using glob, to locate all file names. \n",
    "file_ext = '.csv'\n",
    "files = sorted([file for file in glob.glob(f'*{file_ext}')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9fbce328-28a8-4086-8f02-8992f4b70bed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first five files : ['10_nov_20.csv', '11_oct_20.csv', '12_sep_20.csv', '13_aug_20.csv', '14_jul_20.csv'] last 5 files:  ['6_mar_21.csv', '7_feb_21.csv', '8_jan_21.csv', '9_dec_20.csv']\n"
     ]
    }
   ],
   "source": [
    "#the first five in the list to confirm \n",
    "print('first five files :',files[0:5],\n",
    "      'last 5 files: ' ,files[-4:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb248bc2-7abd-4d55-853f-ee706ccca3a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fy/0gjwghjd3qxdc86mx10x01rr0000gp/T/ipykernel_1744/3199323081.py:2: DtypeWarning: Columns (23) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  all_flights = pd.concat([pd.read_csv(file) for file in files ])\n"
     ]
    }
   ],
   "source": [
    "#using the pd.concat() i will read from a list comprehension to concat each and every csv. \n",
    "all_flights = pd.concat([pd.read_csv(file) for file in files ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "36aba3bb-7f71-44c5-bcd2-5b152ecc708b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export to csv\n",
    "all_flights.to_csv( \"all_flights.csv\", index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "29f04e0e-39ed-455b-bd5b-7cdaebc8cb6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(34409230, 34)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_flights.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b465c6-e5ea-4ea4-a2e6-c8bf2d806514",
   "metadata": {},
   "source": [
    "The process is completed with 34,409,230 flights with 34 _raw_ feature columns. It took approximately 8 minutes to process the file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "03d81220-921f-4a62-9a53-bf1dd8d535c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>YEAR</th>\n",
       "      <th>MONTH</th>\n",
       "      <th>DAY_OF_MONTH</th>\n",
       "      <th>DAY_OF_WEEK</th>\n",
       "      <th>FL_DATE</th>\n",
       "      <th>OP_UNIQUE_CARRIER</th>\n",
       "      <th>TAIL_NUM</th>\n",
       "      <th>OP_CARRIER_FL_NUM</th>\n",
       "      <th>ORIGIN</th>\n",
       "      <th>ORIGIN_CITY_NAME</th>\n",
       "      <th>...</th>\n",
       "      <th>DIVERTED</th>\n",
       "      <th>CRS_ELAPSED_TIME</th>\n",
       "      <th>FLIGHTS</th>\n",
       "      <th>DISTANCE</th>\n",
       "      <th>CARRIER_DELAY</th>\n",
       "      <th>WEATHER_DELAY</th>\n",
       "      <th>NAS_DELAY</th>\n",
       "      <th>SECURITY_DELAY</th>\n",
       "      <th>LATE_AIRCRAFT_DELAY</th>\n",
       "      <th>Unnamed: 33</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>2020-11-12</td>\n",
       "      <td>AA</td>\n",
       "      <td>N844NN</td>\n",
       "      <td>1783</td>\n",
       "      <td>PHL</td>\n",
       "      <td>Philadelphia, PA</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>235.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1303.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020</td>\n",
       "      <td>11</td>\n",
       "      <td>13</td>\n",
       "      <td>5</td>\n",
       "      <td>2020-11-13</td>\n",
       "      <td>AA</td>\n",
       "      <td>N339PL</td>\n",
       "      <td>1783</td>\n",
       "      <td>PHL</td>\n",
       "      <td>Philadelphia, PA</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>235.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1303.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020</td>\n",
       "      <td>11</td>\n",
       "      <td>14</td>\n",
       "      <td>6</td>\n",
       "      <td>2020-11-14</td>\n",
       "      <td>AA</td>\n",
       "      <td>N879NN</td>\n",
       "      <td>1783</td>\n",
       "      <td>PHL</td>\n",
       "      <td>Philadelphia, PA</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>235.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1303.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020</td>\n",
       "      <td>11</td>\n",
       "      <td>15</td>\n",
       "      <td>7</td>\n",
       "      <td>2020-11-15</td>\n",
       "      <td>AA</td>\n",
       "      <td>N829NN</td>\n",
       "      <td>1783</td>\n",
       "      <td>PHL</td>\n",
       "      <td>Philadelphia, PA</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>235.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1303.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020</td>\n",
       "      <td>11</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>2020-11-16</td>\n",
       "      <td>AA</td>\n",
       "      <td>N982AN</td>\n",
       "      <td>1783</td>\n",
       "      <td>PHL</td>\n",
       "      <td>Philadelphia, PA</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>235.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1303.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   YEAR  MONTH  DAY_OF_MONTH  DAY_OF_WEEK     FL_DATE OP_UNIQUE_CARRIER  \\\n",
       "0  2020     11            12            4  2020-11-12                AA   \n",
       "1  2020     11            13            5  2020-11-13                AA   \n",
       "2  2020     11            14            6  2020-11-14                AA   \n",
       "3  2020     11            15            7  2020-11-15                AA   \n",
       "4  2020     11            16            1  2020-11-16                AA   \n",
       "\n",
       "  TAIL_NUM  OP_CARRIER_FL_NUM ORIGIN  ORIGIN_CITY_NAME  ... DIVERTED  \\\n",
       "0   N844NN               1783    PHL  Philadelphia, PA  ...      0.0   \n",
       "1   N339PL               1783    PHL  Philadelphia, PA  ...      0.0   \n",
       "2   N879NN               1783    PHL  Philadelphia, PA  ...      0.0   \n",
       "3   N829NN               1783    PHL  Philadelphia, PA  ...      0.0   \n",
       "4   N982AN               1783    PHL  Philadelphia, PA  ...      0.0   \n",
       "\n",
       "  CRS_ELAPSED_TIME FLIGHTS DISTANCE  CARRIER_DELAY  WEATHER_DELAY  NAS_DELAY  \\\n",
       "0            235.0     1.0   1303.0            NaN            NaN        NaN   \n",
       "1            235.0     1.0   1303.0            NaN            NaN        NaN   \n",
       "2            235.0     1.0   1303.0            NaN            NaN        NaN   \n",
       "3            235.0     1.0   1303.0            NaN            NaN        NaN   \n",
       "4            235.0     1.0   1303.0            NaN            NaN        NaN   \n",
       "\n",
       "   SECURITY_DELAY  LATE_AIRCRAFT_DELAY  Unnamed: 33  \n",
       "0             NaN                  NaN          NaN  \n",
       "1             NaN                  NaN          NaN  \n",
       "2             NaN                  NaN          NaN  \n",
       "3             NaN                  NaN          NaN  \n",
       "4             NaN                  NaN          NaN  \n",
       "\n",
       "[5 rows x 34 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_flights.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30fed352-63dd-40ab-84a4-f045f8d77464",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "-----\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c76bf66-de0e-4ff9-aa54-54cbbd0ea87a",
   "metadata": {},
   "source": [
    "#### Using the Amadeus flight price analysis API\n",
    "---\n",
    "Amadeus, a transportation global distribution system*, provides developers with several very useful self-service API's to access current and historical data relating to flights and much more. \n",
    "<br><br>\n",
    "For this project I wanted to provide the user the cost of a potential delay. This data will be used in a secondary regression that explains how much the added time will cost. \n",
    "<br><br>\n",
    "To accomplish this price estimate I needed prices for each flight**. We, as travelers, all know that not every seat costs the same amount of money and that pricing conducted by the airline are done dynamically as a result of each airline's pricing strategy. \n",
    "\n",
    "\n",
    "---\n",
    "References:<br>\n",
    "[Flight Price Analysis API](https://developers.amadeus.com/self-service/category/air/api-doc/flight-price-analysis)\n",
    "\\** The total quota is 10,000 calls before having to use a production price tier. \n",
    "\n",
    "Notes:<br>\n",
    "\\* Definition of a [global distribution system (GDS)](https://en.wikipedia.org/wiki/Global_distribution_system):\n",
    "> \"is a computerised network system owned or operated by a company that enables transactions between travel industry service providers, mainly airlines, hotels, car rental companies, and travel agencies.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a3a15b-603b-47ba-b00e-5141b18779fa",
   "metadata": {},
   "source": [
    "To test the API, I will submit their example code for a flight_offer_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "db3e3901-d36c-48e4-a50d-c0bdb900fe36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "[{'type': 'itinerary-price-metric', 'origin': {'iataCode': 'LAX'}, 'destination': {'iataCode': 'JFK'}, 'departureDate': '2019-12-31', 'transportType': 'FLIGHT', 'currencyCode': 'EUR', 'oneWay': False, 'priceMetrics': [{'amount': '63.86', 'quartileRanking': 'MINIMUM'}, {'amount': '424.67', 'quartileRanking': 'FIRST'}, {'amount': '493.59', 'quartileRanking': 'MEDIUM'}, {'amount': '552.25', 'quartileRanking': 'THIRD'}, {'amount': '582.42', 'quartileRanking': 'MAXIMUM'}]}]\n"
     ]
    }
   ],
   "source": [
    "amadeus = Client(\n",
    "    client_id='fYArxk7F2FGo8kJIJpUsJIEP18pDNZHk',\n",
    "    client_secret='dd2tznw3kZaGGnoW'\n",
    ")\n",
    "\n",
    "try:\n",
    "    '''\n",
    "    Returns price metrics of a given itinerary\n",
    "    '''\n",
    "    response = amadeus.analytics.itinerary_price_metrics.get(originIataCode='LAX',\n",
    "                                                             destinationIataCode='JFK',\n",
    "                                                             departureDate='2019-12-31')\n",
    "    print(response.status_code)\n",
    "    print(response.data)\n",
    "except ResponseError as error:\n",
    "    raise error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b11f37-a88b-4043-97d6-e27f8ba7da95",
   "metadata": {},
   "source": [
    "If you get a rather large JSON returned then you have successfully accessed the self-service. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7dc1c34-8dae-4369-bee1-d2489e2bbd24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c35dc2f-db87-4bc1-8c2b-147d3db82158",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79bd9b25-1198-46ff-ae81-165b6678b61c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0471a237-9337-452e-a133-e1be8b7b3e66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ca19d2-8a6a-40d5-8a77-e518b09d3e53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb0a904-5d98-407b-95bc-7dd8c3baf26a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "037a4e6f-1757-4bd1-a91e-ac7fce6f240c",
   "metadata": {},
   "source": [
    "### Data Size\n",
    "---\n",
    "As you can see above, doing any operations on 34.4 million rows of data would be _taxing_ on anyone's local system. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2774dfa8-d5bc-4e8a-826f-4bda39d19271",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
